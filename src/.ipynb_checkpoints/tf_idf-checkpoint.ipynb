{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "TRAIN_FILE = \"../resources/data/train_tweets.txt\"\n",
    "SML_TRAIN_FILE = \"../resources/data/test.txt\"\n",
    "TEST_FILE = \"../resources/data/test_tweets_unlabeled.txt\"\n",
    "\n",
    "GLOVE_25D = \"../resources/glove/glove.twitter.27B.25d.txt\"\n",
    "GLOVE_200D = \"../resources/glove/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all RTs (reweets)\n",
    "def filter_RT(df):\n",
    "    rt = df['Text'].str.startswith('RT @handle')\n",
    "    not_rt = [not i for i in rt]\n",
    "    result_df = df[not_rt]\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special terms like \"@handle\", links\n",
    "def rmv_special_term(df, rmv_all_spec=False):\n",
    "    # remove @s\n",
    "    result_df = df.replace(to_replace ='@handle', value = '@handle', regex=True)\n",
    "    # remove # but save tags\n",
    "    result_df = result_df.replace(to_replace ='#', value = '', regex=True)\n",
    "    # remove links and urls\n",
    "    result_df = result_df.replace(to_replace ='\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', value = '@url', regex=True)\n",
    "    \n",
    "    # filter out all chars except 1-9/a-z/A-Z, such as :-( ' , . / \\ \n",
    "    if rmv_all_spec:\n",
    "        result_df = result_df.replace(to_replace ='([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', value = '', regex=True)\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# main call\n",
    "def preprocess(df, rmv_rt=True, rmv_all_spec=False, rmv_stop=False, lemmatize=False, add_pos=False):\n",
    "    logging.info('Preprocess starting')\n",
    "    \n",
    "    if rmv_rt:\n",
    "        df = filter_RT(df)\n",
    "    else:\n",
    "        df = df.replace(to_replace ='^RT @handle.*', value = '@rt', regex=True)\n",
    "    \n",
    "    result_df = rmv_special_term(df, rmv_all_spec)\n",
    "    \n",
    "    result_df['Text'] = result_df['Text'].apply(lambda x: x.lower().rstrip().lstrip())\n",
    "    \n",
    "    # tokenize sentence\n",
    "    tknzr = TweetTokenizer()\n",
    "    result_df['Text'] = result_df['Text'].apply(lambda x: tknzr.tokenize(x))\n",
    "    \n",
    "    # add pos tags\n",
    "    if add_pos:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: np.asarray(nltk.pos_tag(x)))\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: x.ravel())\n",
    "        \n",
    "    # remove stop words\n",
    "    if rmv_stop:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: [i for i in x if i not in cached_stop_words])\n",
    "    \n",
    "    # stem words\n",
    "    if lemmatize:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: [lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in x])\n",
    "    \n",
    "    logging.info('Preprocess ending')  \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all tweets from same user\n",
    "def merge_tweets(df):\n",
    "    df['Text'] = df['Text'].apply(lambda x: ''.join(i + ' ' for i in x))\n",
    "    \n",
    "    aggregation_functions = {'Text': ''.join}\n",
    "    result_df = df.groupby(df['ID']).aggregate(aggregation_functions).reset_index()\n",
    "    \n",
    "    df['Text'] = df['Text'].apply(lambda x: x.rstrip())\n",
    "    print(\"finish merge tweets\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tf-idf features\n",
    "def tf_idf(train_df, test_df):\n",
    "    #get the text column \n",
    "    train_docs = train_df['Text'].tolist()\n",
    "    test_docs = test_df['Text'].tolist()\n",
    "    \n",
    "    #create a vocabulary of words, \n",
    "    #ignore words that appear in max_df of documents\n",
    "    stop_words = stopwords.words('english')\n",
    "    cv = CountVectorizer(max_df=0.85, stop_words=stop_words, decode_error='ignore')\n",
    "    trian_wc_vec = cv.fit_transform(train_docs)\n",
    "    test_wc_vec = cv.transform(test_docs)\n",
    "    \n",
    "    # get tfidf\n",
    "    transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "    tfidf_train_df = transformer.fit_transform(trian_wc_vec)\n",
    "    tfidf_test_df = transformer.transform(test_wc_vec)\n",
    "    \n",
    "    print(\"Finish tf-idf feature extraction\")\n",
    "\n",
    "    return tfidf_train_df, tfidf_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lexicon features\n",
    "def add_lexicon_features(df, feature_vec=None):\n",
    "    features = []\n",
    "    for index, row in df.iterrows():\n",
    "        text, feature = row['Text'], []\n",
    "        feature.extend(avg_var_word_len(text))\n",
    "        feature.append(len(text))\n",
    "        \n",
    "        features.append(feature)\n",
    "    \n",
    "    A = np.array(features)\n",
    "    A = normalize(A, axis=0, norm='max')\n",
    "    \n",
    "    if feature_vec == None:\n",
    "        print(\"finish add lexicon features\")\n",
    "        return sparse.csr_matrix(A)\n",
    "    else:\n",
    "        for column in A.T: \n",
    "            feature_vec = sparse.hstack((feature_vec, column[:,None]))\n",
    "        \n",
    "        print(\"finish add lexicon features\")\n",
    "        return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caculate averge word length for given sentence (word should starts with alphabet letters)\n",
    "def avg_var_word_len(text):\n",
    "    words, length = text.split(' '), []\n",
    "    \n",
    "    for word in words:\n",
    "        if word[0].isalpha() and word[0].islower():\n",
    "            length.append(len(word))\n",
    "            \n",
    "    length = np.array(length)\n",
    "    \n",
    "    if length.size == 0:\n",
    "        return [0, 0, 0]\n",
    "    \n",
    "    return [np.mean(length), np.std(length), np.median(length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate substring for df\n",
    "def generate_substring(df, length=6):\n",
    "    iter_df = df.copy()\n",
    "    \n",
    "    for index, row in iter_df.iterrows():\n",
    "        text, new_words = row['Text'], []\n",
    "        words = text.split(' ')\n",
    "        \n",
    "        for word in words: \n",
    "            if len(word) >= length:\n",
    "                for i in range(0, len(word)-length+1):\n",
    "                    sub_word = word[i:i + length]\n",
    "                    new_words.append(sub_word)\n",
    "                \n",
    "            new_words.append(word)\n",
    "        \n",
    "        new_text = ''.join(i + ' ' for i in new_words).rstrip()\n",
    "        \n",
    "        df.loc[index, 'Text'] = new_text\n",
    "    \n",
    "    print(\"Finish substring extraction\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "models = [svm.LinearSVC(C=0.68, max_iter=1000)]\n",
    "         # MultinomialNB()]\n",
    "\n",
    "titles = ['LinearSVM, 0.68']\n",
    "          #'MNB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_tf_idf(df, merge=False, add_lexicon=False, substring=False, substring_len=3):\n",
    "    cv = KFold(n_splits=10, random_state=90051, shuffle=True)\n",
    "    \n",
    "    scores = {}\n",
    "    for train_index, test_index in cv.split(df):\n",
    "        train_df, test_df = df.iloc[train_index].reset_index(drop=True), df.iloc[test_index].reset_index(drop=True)\n",
    "        \n",
    "        # merge all tweets from same user to one document string\n",
    "        if merge:\n",
    "            train_df = merge_tweets(train_df)\n",
    "        else:\n",
    "            train_df['Text'] = train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "        test_df['Text'] = test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "        if substring:\n",
    "            train_df = generate_substring(train_df, length=substring_len)\n",
    "            test_df = generate_substring(test_df, length=substring_len)\n",
    "        \n",
    "        X_train, X_test = tf_idf(train_df, test_df)\n",
    "        y_train, y_test = train_df['ID'], test_df['ID']\n",
    "        \n",
    "        if add_lexicon:\n",
    "            X_train = add_lexicon_features(train_df, feature_vec=X_train)\n",
    "            X_test = add_lexicon_features(test_df, feature_vec=X_test)\n",
    "        \n",
    "        for title, model in zip(titles, models):\n",
    "            model.fit(X_train, y_train)\n",
    "            acc = accuracy_score(model.predict(X_test), y_test)\n",
    "            \n",
    "            if title in scores.keys():\n",
    "                scores[title] += acc\n",
    "            else:\n",
    "                scores[title] = acc\n",
    "        \n",
    "    for title in titles:\n",
    "        acc = scores[title] / 10\n",
    "        print(\"####INFO: trainning\", title, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf(train_df, test_df, merge=False, add_lexicon=False, substring=False, substring_len=3):\n",
    "    # merge all tweets from same user to one document string\n",
    "    if merge:\n",
    "        train_df = merge_tweets(train_df)\n",
    "    else:\n",
    "        train_df['Text'] = train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "    test_df['Text'] = test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "    \n",
    "    if substring:\n",
    "        train_df = generate_substring(train_df, length=substring_len)\n",
    "        test_df = generate_substring(test_df, length=substring_len)\n",
    "        \n",
    "    X_train, X_test = tf_idf(train_df, test_df)\n",
    "    y_train = train_df['ID']\n",
    "    \n",
    "    if add_lexicon:\n",
    "        X_train = add_lexicon_features(train_df, feature_vec=X_train)\n",
    "        X_test = add_lexicon_features(test_df, feature_vec=X_test)\n",
    "    \n",
    "    for title, model in zip(titles, models):\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"model fit finished\")\n",
    "        label = model.predict(X_test)\n",
    "        print(\"model predict finished\")\n",
    "    \n",
    "        wtr = csv.writer(open('prediction_' + title + '.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "        wtr.writerow(['Id','Predicted'])\n",
    "        for i in range(0, label.size):\n",
    "            wtr.writerow([i+1, label[i]])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading\n",
      "Finsih preprocess\n",
      "Finish tf-idf feature extraction\n",
      "Finish tf-idf feature extraction\n",
      "Finish tf-idf feature extraction\n",
      "Finish tf-idf feature extraction\n",
      "Finish tf-idf feature extraction\n",
      "Finish tf-idf feature extraction\n",
      "Finish tf-idf feature extraction\n",
      "Finish tf-idf feature extraction\n",
      "Finish tf-idf feature extraction\n",
      "Finish tf-idf feature extraction\n",
      "####INFO: trainning LinearSVM, 0.66 0.45254237288135596\n",
      "####INFO: trainning LinearSVM, 0.67 0.45264830508474585\n",
      "####INFO: trainning LinearSVM, 0.68 0.45275423728813563\n",
      "####INFO: trainning LinearSVM, 0.69 0.45264830508474574\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "raw_train_df = pd.read_csv(TRAIN_FILE, delimiter='\\t', header=None, names=['ID','Text'])\n",
    "raw_test_df = pd.read_csv(TEST_FILE, delimiter='\\t', header=None, names=['Text'])\n",
    "# print(train_df.shape)\n",
    "# print(test_df.shape)\n",
    "\n",
    "print(\"Finish reading\")\n",
    "\n",
    "preprocess_train_df = preprocess(raw_train_df, rmv_rt=True, rmv_all_spec=False, rmv_stop=False, lemmatize=False, add_pos=True)\n",
    "preprocess_test_df = preprocess(raw_test_df, rmv_rt=False, rmv_all_spec=False, rmv_stop=False, lemmatize=False, add_pos=True)\n",
    "#print(preprocess_train_df.shape, preprocess_test_df.shape)\n",
    "\n",
    "print(\"Finsih preprocess\")\n",
    "\n",
    "'''\n",
    "merged_train_df = merge_tweets(preprocess_train_df)\n",
    "merged_test_df = preprocess_test_df\n",
    "merged_train_df['Text'] = merged_train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "merged_test_df['Text'] = merged_test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "\n",
    "tf_idf_train, tf_idf_test = tf_idf(merged_train_df, merged_test_df)\n",
    "print(tf_idf_train.shape, tf_idf_test.shape)\n",
    "print(tf_idf_train)\n",
    "'''\n",
    "\n",
    "# cross_validate_tf_idf(preprocess_train_df, merge=False, add_lexicon=False, substring=False, substring_len=3)\n",
    "\n",
    "predict_tf_idf(preprocess_train_df, preprocess_test_df, merge=False, add_lexicon=False, substring=False, substring_len=3)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
