{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "TRAIN_FILE = \"../resources/data/train_tweets.txt\"\n",
    "SML_TRAIN_FILE = \"../resources/data/test.txt\"\n",
    "TEST_FILE = \"../resources/data/test_tweets_unlabeled.txt\"\n",
    "\n",
    "GLOVE_25D = \"../resources/glove/glove.twitter.27B.25d.txt\"\n",
    "GLOVE_200D = \"../resources/glove/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all RTs (reweets)\n",
    "def filter_RT(df):\n",
    "    rt = df['Text'].str.startswith('RT @handle')\n",
    "    not_rt = [not i for i in rt]\n",
    "    result_df = df[not_rt]\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special terms like \"@handle\", links\n",
    "def rmv_special_term(df, rmv_all_spec=False):\n",
    "    # remove @s\n",
    "    result_df = df.replace(to_replace ='@handle', value = '@handle', regex=True)\n",
    "    # remove # but save tags\n",
    "    result_df = result_df.replace(to_replace ='#', value = '', regex=True)\n",
    "    # remove links and urls\n",
    "    result_df = result_df.replace(to_replace ='\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', value = '@url', regex=True)\n",
    "    \n",
    "    # filter out all chars except 1-9/a-z/A-Z, such as :-( ' , . / \\ \n",
    "    if rmv_all_spec:\n",
    "        result_df = result_df.replace(to_replace ='([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', value = '', regex=True)\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# main call\n",
    "def preprocess(df, rmv_rt=True, rmv_all_spec=False, rmv_stop=False, lemmatize=False, word_ngram=[1], add_pos=False, pos_ngram=[1]):\n",
    "    logging.info('Preprocess starting')\n",
    "    \n",
    "    if rmv_rt:\n",
    "        df = filter_RT(df)\n",
    "    else:\n",
    "        df = df.replace(to_replace ='RT @handle', value = '@RT @handle', regex=True)\n",
    "    \n",
    "    result_df = rmv_special_term(df, rmv_all_spec)\n",
    "    \n",
    "    result_df['Text'] = result_df['Text'].apply(lambda x: x.rstrip().lstrip())\n",
    "    \n",
    "    # tokenize sentence\n",
    "    tknzr = TweetTokenizer()\n",
    "    result_df['Text'] = result_df['Text'].apply(lambda x: tknzr.tokenize(x))\n",
    "    \n",
    "    # add pos tags\n",
    "    if add_pos:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: pos_analysis(x, ngram=pos_ngram))\n",
    "        \n",
    "    if len(word_ngram) > 0:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: get_word_ngram(x, ngram=word_ngram))\n",
    "         \n",
    "    # remove stop words\n",
    "    if rmv_stop:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: [i for i in x if i not in cached_stop_words])\n",
    "    \n",
    "    # stem words\n",
    "    if lemmatize:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: [lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in x])\n",
    "    \n",
    "    logging.info('Preprocess ending')  \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis POS (part of speech) and ngram\n",
    "def pos_analysis(word_list, ngram=[1]):\n",
    "    if 0 in ngram:\n",
    "        return word_list\n",
    "    \n",
    "    postag_word = np.asarray(nltk.pos_tag(word_list))\n",
    "    \n",
    "    # get only pos tags\n",
    "    pos_tags = []\n",
    "    for chunk in postag_word:\n",
    "        pos_tags.append(chunk[1])\n",
    "    \n",
    "    result = word_list\n",
    "    for n in ngram:\n",
    "        pos_ngrams = []\n",
    "        \n",
    "        if n == 1:\n",
    "            pos_ngrams = pos_tags\n",
    "        else:\n",
    "            if n >= len(pos_tags):\n",
    "                pos_ngrams.append(''.join(tag for tag in pos_tags))\n",
    "            else:\n",
    "                for i in range(0, len(pos_tags)-n+1):\n",
    "                    new_tag = ''\n",
    "                    for j in range(i, i+n):\n",
    "                        new_tag += str(pos_tags[j])\n",
    "                \n",
    "                    pos_ngrams.append(new_tag)\n",
    "        \n",
    "        result = np.append(result, [i for i in pos_ngrams])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word ngram\n",
    "def get_word_ngram(raw_word_list, ngram=[1]):\n",
    "    result = []\n",
    "    # get pure word list, eliminate pos tags\n",
    "    pure_word_list = []\n",
    "    for word in raw_word_list:\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "        \n",
    "        if not word[0].isupper():\n",
    "            pure_word_list.append(word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "            \n",
    "    if 0 in ngram:\n",
    "        return result\n",
    "    \n",
    "    for n in ngram:\n",
    "        if n == 1:\n",
    "            result = np.append(result, pure_word_list)\n",
    "        else:\n",
    "            if n >= len(pure_word_list):\n",
    "                result = np.append(result, ''.join(word for word in pure_word_list))\n",
    "            else:\n",
    "                for i in range(0, len(pure_word_list)-n+1):\n",
    "                    new_tag = ''\n",
    "                    for j in range(i, i+n):\n",
    "                        new_tag += str(pure_word_list[j])\n",
    "                \n",
    "                    result = np.append(result, new_tag)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all tweets from same user\n",
    "def merge_tweets(df):\n",
    "    df['Text'] = df['Text'].apply(lambda x: ''.join(i + ' ' for i in x))\n",
    "    \n",
    "    aggregation_functions = {'Text': ''.join}\n",
    "    result_df = df.groupby(df['ID']).aggregate(aggregation_functions).reset_index()\n",
    "    \n",
    "    df['Text'] = df['Text'].apply(lambda x: x.rstrip())\n",
    "    print(\"finish merge tweets\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tf-idf features\n",
    "def tf_idf(train_df, test_df):\n",
    "    #get the text column \n",
    "    train_docs = train_df['Text'].tolist()\n",
    "    test_docs = test_df['Text'].tolist()\n",
    "    \n",
    "    #create a vocabulary of words, \n",
    "    #ignore words that appear in max_df of documents\n",
    "    cv = CountVectorizer(max_df=0.42, min_df=1, decode_error='ignore')\n",
    "    trian_wc_vec = cv.fit_transform(train_docs)\n",
    "    test_wc_vec = cv.transform(test_docs)\n",
    "    \n",
    "    # get tfidf\n",
    "    transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "    tfidf_train_df = transformer.fit_transform(trian_wc_vec)\n",
    "    tfidf_test_df = transformer.transform(test_wc_vec)\n",
    "    \n",
    "    #print(\"Finish tf-idf feature extraction\")\n",
    "\n",
    "    return tfidf_train_df, tfidf_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lexicon features\n",
    "def add_lexicon_features(df, feature_vec=None):\n",
    "    features = []\n",
    "    for index, row in df.iterrows():\n",
    "        text, feature = row['Text'], []\n",
    "        feature.extend(avg_var_word_len(text))\n",
    "        feature.append(len(text))\n",
    "        \n",
    "        features.append(feature)\n",
    "    \n",
    "    A = np.array(features)\n",
    "    A = normalize(A, axis=0, norm='max')\n",
    "    \n",
    "    if feature_vec == None:\n",
    "        print(\"finish add lexicon features\")\n",
    "        return sparse.csr_matrix(A)\n",
    "    else:\n",
    "        for column in A.T: \n",
    "            feature_vec = sparse.hstack((feature_vec, column[:,None]))\n",
    "        \n",
    "        print(\"finish add lexicon features\")\n",
    "        return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caculate averge word length for given sentence (word should starts with alphabet letters)\n",
    "def avg_var_word_len(text):\n",
    "    words, length = text.split(' '), []\n",
    "    \n",
    "    for word in words:\n",
    "        if word[0].isalpha() and word[0].islower():\n",
    "            length.append(len(word))\n",
    "            \n",
    "    length = np.array(length)\n",
    "    \n",
    "    if length.size == 0:\n",
    "        return [0, 0, 0]\n",
    "    \n",
    "    return [np.mean(length), np.std(length), np.median(length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate substring for df\n",
    "def generate_substring(df, length=6):\n",
    "    iter_df = df.copy()\n",
    "    \n",
    "    for index, row in iter_df.iterrows():\n",
    "        text, new_words = row['Text'], []\n",
    "        words = text.split(' ')\n",
    "        \n",
    "        for word in words: \n",
    "            if len(word) >= length:\n",
    "                for i in range(0, len(word)-length+1):\n",
    "                    sub_word = word[i:i + length]\n",
    "                    new_words.append(sub_word)\n",
    "                \n",
    "            new_words.append(word)\n",
    "        \n",
    "        new_text = ''.join(i + ' ' for i in new_words).rstrip()\n",
    "        \n",
    "        df.loc[index, 'Text'] = new_text\n",
    "    \n",
    "    #print(\"Finish substring extraction\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# sentiment analysis\n",
    "def sentiment_analysis(df, feature_vec=None):\n",
    "    sentiment_scores = []\n",
    "    for index, row in df.iterrows():\n",
    "        text, ori_words = row['Text'], []\n",
    "        words = text.split(' ')\n",
    "        \n",
    "        for word in words:\n",
    "            if '@rt' in word:\n",
    "                break\n",
    "                \n",
    "            if len(word) < 1:\n",
    "                continue\n",
    "                \n",
    "            ori_words.append(word)\n",
    "                \n",
    "        sentence = ''.join(i + ' ' for i in ori_words).rstrip()\n",
    "        score = analyser.polarity_scores(sentence)\n",
    "        sentiment_scores.append([score.get('neg'), score.get('neu'), score.get('pos'), score.get('compound')])\n",
    "        \n",
    "    A = np.array(sentiment_scores)\n",
    "    if feature_vec == None:\n",
    "        #print(\"finish add sentiment features\")\n",
    "        return sparse.csr_matrix(A)\n",
    "    else:\n",
    "        for column in A.T: \n",
    "            feature_vec = sparse.hstack((feature_vec, column[:,None]))\n",
    "        \n",
    "        #print(\"finish add sentiment features\")\n",
    "        return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduction(X_train, X_test):\n",
    "    pca = TruncatedSVD(n_components=1000)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    print(\"finish pca reduction\")\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "models = [svm.LinearSVC(C=0.68, max_iter=1000)]\n",
    "          #LogisticRegression(solver=\"lbfgs\", penalty=\"l2\", C=1e-2, multi_class='auto', max_iter=1000, fit_intercept= False)]\n",
    "         #XGBClassifier(random_state=42, seed=2, colsample_bytree=0.6, subsample=0.7, max_depth=3, n_estimators=300, learning_rate=0.1)]\n",
    "         # MultinomialNB()]\n",
    "\n",
    "titles = ['LinearSVM, 0.68']\n",
    "          #'LogisticRegression']\n",
    "         #'XGBoost']\n",
    "          #'MNB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_tf_idf(df, merge=False, add_lexicon=False, substring=False, substring_len=3, add_sentiment=False, pca=False):\n",
    "    cv = KFold(n_splits=10, random_state=90051, shuffle=True)\n",
    "    \n",
    "    scores = {}\n",
    "    for train_index, test_index in cv.split(df):\n",
    "        train_df, test_df = df.iloc[train_index].reset_index(drop=True), df.iloc[test_index].reset_index(drop=True)\n",
    "        \n",
    "        # merge all tweets from same user to one document string\n",
    "        if merge:\n",
    "            train_df = merge_tweets(train_df)\n",
    "        else:\n",
    "            train_df['Text'] = train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "        test_df['Text'] = test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "        if substring:\n",
    "            train_df = generate_substring(train_df, length=substring_len)\n",
    "            test_df = generate_substring(test_df, length=substring_len)\n",
    "        \n",
    "        X_train, X_test = tf_idf(train_df, test_df)\n",
    "        y_train, y_test = train_df['ID'], test_df['ID']\n",
    "        \n",
    "        if add_lexicon:\n",
    "            X_train = add_lexicon_features(train_df, feature_vec=X_train)\n",
    "            X_test = add_lexicon_features(test_df, feature_vec=X_test)\n",
    "            \n",
    "        if add_sentiment:\n",
    "            X_train = sentiment_analysis(train_df, feature_vec=X_train)\n",
    "            X_test = sentiment_analysis(test_df, feature_vec=X_test)\n",
    "            \n",
    "        if pca:\n",
    "            X_train, X_test = pca_reduction(X_train, X_test)\n",
    "        \n",
    "        for title, model in zip(titles, models):\n",
    "            model.fit(X_train, y_train)\n",
    "            predicted_labels = model.predict(X_test)\n",
    "            acc = accuracy_score(predicted_labels, y_test)\n",
    "            \n",
    "            # uncomment to print miss labeled data\n",
    "            # for i in range(0, len(predicted_labels)):\n",
    "            #   if predicted_labels[i] != y_test[i]:\n",
    "            #        print(\"#\" + str(i) + \"; T: \" + str(y_test[i]) + \"; F: \" + str(predicted_labels[i]) + \"; Text: \" + test_df.loc[i,'Text'])\n",
    "            \n",
    "            if title in scores.keys():\n",
    "                scores[title] += acc\n",
    "            else:\n",
    "                scores[title] = acc\n",
    "        \n",
    "    for title in titles:\n",
    "        acc = scores[title] / 10\n",
    "        print(\"####INFO: trainning\", title, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf(train_df, test_df, merge=False, add_lexicon=False, substring=False, substring_len=3, add_sentiment=False):\n",
    "    # merge all tweets from same user to one document string\n",
    "    if merge:\n",
    "        train_df = merge_tweets(train_df)\n",
    "    else:\n",
    "        train_df['Text'] = train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "    test_df['Text'] = test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "    \n",
    "    if substring:\n",
    "        train_df = generate_substring(train_df, length=substring_len)\n",
    "        test_df = generate_substring(test_df, length=substring_len)\n",
    "        \n",
    "    X_train, X_test = tf_idf(train_df, test_df)\n",
    "    y_train = train_df['ID']\n",
    "    \n",
    "    if add_lexicon:\n",
    "        X_train = add_lexicon_features(train_df, feature_vec=X_train)\n",
    "        X_test = add_lexicon_features(test_df, feature_vec=X_test)\n",
    "        \n",
    "    if add_sentiment:\n",
    "        X_train = sentiment_analysis(train_df, feature_vec=X_train)\n",
    "        X_test = sentiment_analysis(test_df, feature_vec=X_test)\n",
    "    \n",
    "    for title, model in zip(titles, models):\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"model \" + title + \" fit finished\")\n",
    "        label = model.predict(X_test)\n",
    "        print(\"model predict finished\")\n",
    "    \n",
    "        wtr = csv.writer(open('prediction_' + title + '.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "        wtr.writerow(['Id','Predicted'])\n",
    "        for i in range(0, label.size):\n",
    "            wtr.writerow([i+1, label[i]])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, train_df, test_df, wordngram=[1], pos=False, posngram=[1], addsentiment=True):\n",
    "    train_df = preprocess(train_df, rmv_rt=False, rmv_all_spec=False, rmv_stop=False, lemmatize=False, word_ngram=wordngram, add_pos=pos, pos_ngram=posngram)\n",
    "    test_df = preprocess(test_df, rmv_rt=False, rmv_all_spec=False, rmv_stop=False, lemmatize=False, word_ngram=wordngram, add_pos=pos, pos_ngram=posngram)\n",
    "    \n",
    "    train_df['Text'] = train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "    test_df['Text'] = test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "    \n",
    "    X_train, X_test = tf_idf(train_df, test_df)\n",
    "    y_train, y_test = train_df['ID'], test_df['ID']\n",
    "    \n",
    "    if addsentiment:\n",
    "        X_train = sentiment_analysis(train_df, feature_vec=X_train)\n",
    "        X_test = sentiment_analysis(test_df, feature_vec=X_test)\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "    train_labels = model.predict(X_train)\n",
    "    predicted_labels = model.predict(X_test)\n",
    "    \n",
    "    return np.array(train_labels).reshape(-1,1), np.array(predicted_labels).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_cross_validate(raw_df, add_sentiment=True):\n",
    "    cv = KFold(n_splits=10, random_state=90051, shuffle=True)\n",
    "    \n",
    "    score = 0\n",
    "    for train_index, test_index in cv.split(raw_df):\n",
    "        train_df, test_df = raw_df.iloc[train_index].reset_index(drop=True), raw_df.iloc[test_index].reset_index(drop=True)\n",
    "        y_train, y_test = train_df['ID'], test_df['ID']\n",
    "        \n",
    "        svm_model = svm.LinearSVC(C=0.68, max_iter=1000)\n",
    "        \n",
    "        train_1, test_1 = predict(svm_model, train_df, test_df, wordngram=[1], pos=True, posngram=[1], addsentiment=True)\n",
    "        train_2, test_2 = predict(svm_model, train_df, test_df, wordngram=[2], pos=False, posngram=[1], addsentiment=True)\n",
    "        train_3, test_3 = predict(svm_model, train_df, test_df, wordngram=[1], pos=True, posngram=[1000], addsentiment=True)\n",
    "        \n",
    "        h_model = svm.LinearSVC(C=0.68, max_iter=1000)\n",
    "        \n",
    "        X_train, X_test = [], []\n",
    "        for i in range(0, len(train_1)):\n",
    "            X_train.append(str(train_1[i]) + ' ' + str(train_2[i]) + ' ' + str(train_3[i]))\n",
    "        for i in range(0, len(test_1)):\n",
    "            X_test.append(str(test_1[i]) + ' ' + str(test_2[i]) + ' ' + str(test_3[i]))\n",
    "        \n",
    "        X_train = np.array(X_train)\n",
    "        X_test = np.array(X_test)\n",
    "        \n",
    "        stop_words = stopwords.words('english')\n",
    "        cv = CountVectorizer(max_df=0.85, stop_words=stop_words, decode_error='ignore')\n",
    "        trian_wc_vec = cv.fit_transform(X_train)\n",
    "        test_wc_vec = cv.transform(X_test)\n",
    "    \n",
    "        # get tfidf\n",
    "        transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "        X_train = transformer.fit_transform(trian_wc_vec)\n",
    "        X_test = transformer.transform(test_wc_vec)\n",
    "        \n",
    "        h_model.fit(X_train, y_train)\n",
    "        train_acc = accuracy_score(h_model.predict(X_train), y_train)\n",
    "        \n",
    "        predicted_labels = h_model.predict(X_test)\n",
    "        acc = accuracy_score(predicted_labels, y_test)\n",
    "        \n",
    "        sub_acc_1, sub_acc_2, sub_acc_3 = accuracy_score(train_1, y_train), accuracy_score(train_2, y_train), accuracy_score(train_3, y_train)\n",
    "        #print(\"####INFO train error: \", train_acc, sub_acc_1, sub_acc_2, sub_acc_3)\n",
    "        sub_acc_1, sub_acc_2, sub_acc_3 = accuracy_score(test_1, y_test), accuracy_score(test_2, y_test), accuracy_score(test_3, y_test)\n",
    "        #print(\"####INFO test error: \", acc, sub_acc_1, sub_acc_2, sub_acc_3)\n",
    "        \n",
    "        # uncomment to print miss labeled data\n",
    "        # for i in range(0, len(predicted_labels)):\n",
    "        #   if predicted_labels[i] != y_test[i]:\n",
    "        #        print(\"#\" + str(i) + \"; T: \" + str(y_test[i]) + \"; F: \" + str(predicted_labels[i]) + \"; Text: \" + test_df.loc[i,'Text'])\n",
    "        \n",
    "        score += acc\n",
    "        \n",
    "    avg_acc = score / 10\n",
    "    print(\"####INFO: trainning\", 'Stacking', avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading\n",
      "Finsih preprocess\n",
      "####INFO: trainning LinearSVM, 0.68 0.455563290209054\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "raw_train_df = pd.read_csv(SML_TRAIN_FILE, delimiter='\\t', header=None, names=['ID','Text'])\n",
    "# raw_test_df = pd.read_csv(TEST_FILE, delimiter='\\t', header=None, names=['Text'])\n",
    "# print(train_df.shape)\n",
    "# print(test_df.shape)\n",
    "\n",
    "print(\"Finish reading\")\n",
    "preprocess_train_df = preprocess(raw_train_df, rmv_rt=False, rmv_all_spec=False, rmv_stop=False, lemmatize=False, word_ngram=[1], add_pos=True, pos_ngram=[1])\n",
    "# preprocess_test_df = preprocess(raw_test_df, rmv_rt=False, rmv_all_spec=False, rmv_stop=False, lemmatize=False, word_ngram=2, add_pos=True, pos_ngram=1000)\n",
    "#print(preprocess_train_df.shape, preprocess_test_df.shape)\n",
    "\n",
    "print(\"Finsih preprocess\")\n",
    "cross_validate_tf_idf(preprocess_train_df, merge=False, add_lexicon=False, substring=False, substring_len=3, add_sentiment=True, pca=False)\n",
    "\n",
    "#stacking_cross_validate(raw_train_df, add_sentiment=True)\n",
    "\n",
    "#predict_tf_idf(preprocess_train_df, preprocess_test_df, merge=False, add_lexicon=False, substring=False, substring_len=3, add_sentiment=True)\n",
    "#print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
