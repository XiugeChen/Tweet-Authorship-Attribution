{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "TRAIN_FILE = \"../resources/data/train_tweets.txt\"\n",
    "SML_TRAIN_FILE = \"../resources/data/test.txt\"\n",
    "TEST_FILE = \"../resources/data/test_tweets_unlabeled.txt\"\n",
    "\n",
    "GLOVE_25D = \"../resources/glove/glove.twitter.27B.25d.txt\"\n",
    "GLOVE_200D = \"../resources/glove/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all RTs (reweets)\n",
    "def filter_RT(df):\n",
    "    rt = df['Text'].str.startswith('RT @handle')\n",
    "    not_rt = [not i for i in rt]\n",
    "    result_df = df[not_rt]\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special terms like \"@handle\", links\n",
    "def rmv_special_term(df, rmv_all_spec=False):\n",
    "    # remove @s\n",
    "    result_df = df.replace(to_replace ='@handle', value = '@handle', regex=True)\n",
    "    # remove # but save tags\n",
    "    result_df = result_df.replace(to_replace ='#', value = '', regex=True)\n",
    "    # remove links and urls\n",
    "    result_df = result_df.replace(to_replace ='\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', value = '@url', regex=True)\n",
    "    \n",
    "    # filter out all chars except 1-9/a-z/A-Z, such as :-( ' , . / \\ \n",
    "    if rmv_all_spec:\n",
    "        result_df = result_df.replace(to_replace ='([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', value = '', regex=True)\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# main call\n",
    "def preprocess(df, rmv_rt=True, rmv_all_spec=False, rmv_stop=False, lemmatize=False, add_pos=False):\n",
    "    logging.info('Preprocess starting')\n",
    "    \n",
    "    if rmv_rt:\n",
    "        df = filter_RT(df)\n",
    "    else:\n",
    "        df = df.replace(to_replace ='^RT @handle.*', value = '@rt', regex=True)\n",
    "    \n",
    "    result_df = rmv_special_term(df, rmv_all_spec)\n",
    "    \n",
    "    result_df['Text'] = result_df['Text'].apply(lambda x: x.lower().rstrip().lstrip())\n",
    "    \n",
    "    # tokenize sentence\n",
    "    tknzr = TweetTokenizer()\n",
    "    result_df['Text'] = result_df['Text'].apply(lambda x: tknzr.tokenize(x))\n",
    "    \n",
    "    # add pos tags\n",
    "    if add_pos:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: np.asarray(nltk.pos_tag(x)))\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: x.ravel())\n",
    "        \n",
    "    # remove stop words\n",
    "    if rmv_stop:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: [i for i in x if i not in cached_stop_words])\n",
    "    \n",
    "    # stem words\n",
    "    if lemmatize:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: [lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in x])\n",
    "    \n",
    "    logging.info('Preprocess ending')  \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all tweets from same user\n",
    "def merge_tweets(df):\n",
    "    aggregation_functions = {'Text': 'sum'}\n",
    "    result_df = df.groupby(df['ID']).aggregate(aggregation_functions).reset_index()\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tf-idf features\n",
    "def tf_idf(train_df, test_df):\n",
    "    #get the text column \n",
    "    train_docs = train_df['Text'].tolist()\n",
    "    test_docs = test_df['Text'].tolist()\n",
    "    \n",
    "    #create a vocabulary of words, \n",
    "    #ignore words that appear in max_df of documents\n",
    "    stop_words = stopwords.words('english')\n",
    "    cv = CountVectorizer(max_df=0.85, stop_words=stop_words, decode_error='ignore')\n",
    "    trian_wc_vec = cv.fit_transform(train_docs)\n",
    "    test_wc_vec = cv.transform(test_docs)\n",
    "    \n",
    "    # get tfidf\n",
    "    transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "    tfidf_train_df = transformer.fit_transform(trian_wc_vec)\n",
    "    tfidf_test_df = transformer.transform(test_wc_vec)\n",
    "    \n",
    "    print(\"Finish tf-idf feature extraction\")\n",
    "\n",
    "    return tfidf_train_df, tfidf_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "models = [svm.LinearSVC(C=0.6, max_iter=1000)]\n",
    "         # MultinomialNB()]\n",
    "\n",
    "titles = ['LinearSVM, 0.6']\n",
    "          #'MNB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_tf_idf(df, merge=True):\n",
    "    cv = KFold(n_splits=10, random_state=90051, shuffle=True)\n",
    "    \n",
    "    scores = {}\n",
    "    for train_index, test_index in cv.split(df):\n",
    "        train_df, test_df = df.iloc[train_index].reset_index(drop=True), df.iloc[test_index].reset_index(drop=True)\n",
    "        \n",
    "        # merge all tweets from same user to one document string\n",
    "        if merge:\n",
    "            train_df = merge_tweets(train_df)\n",
    "        \n",
    "        train_df['Text'] = train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        test_df['Text'] = test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "        X_train, X_test = tf_idf(train_df, test_df)\n",
    "        y_train, y_test = train_df['ID'], test_df['ID']\n",
    "        \n",
    "        for title, model in zip(titles, models):\n",
    "            model.fit(X_train, y_train)\n",
    "            acc = accuracy_score(model.predict(X_test), y_test)\n",
    "            \n",
    "            if title in scores.keys():\n",
    "                scores[title] += acc\n",
    "            else:\n",
    "                scores[title] = acc\n",
    "        \n",
    "    for title in titles:\n",
    "        acc = scores[title] / 10\n",
    "        print(\"####INFO: trainning\", title, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf(train_df, test_df, merge=True):\n",
    "    # merge all tweets from same user to one document string\n",
    "    if merge:\n",
    "        train_df = merge_tweets(train_df)\n",
    "        \n",
    "    train_df['Text'] = train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "    test_df['Text'] = test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "    X_train, X_test = tf_idf(train_df, test_df)\n",
    "    y_train = train_df['ID']\n",
    "    \n",
    "    for title, model in zip(titles, models):\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"model fit finished\")\n",
    "        label = model.predict(X_test)\n",
    "        print(\"model predict finished\")\n",
    "    \n",
    "        wtr = csv.writer(open('prediction_' + title + '.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "        wtr.writerow(['Id','Predicted'])\n",
    "        for i in range(0, label.size):\n",
    "            wtr.writerow([i+1, label[i]])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading\n",
      "Finsih preprocess\n",
      "####INFO: trainning LinearSVM, 0.1 0.4233050847457627\n",
      "####INFO: trainning LinearSVM, 0.15 0.43400423728813553\n",
      "####INFO: trainning LinearSVM, 0.2 0.4388771186440678\n",
      "####INFO: trainning LinearSVM, 0.25 0.4427966101694915\n",
      "####INFO: trainning LinearSVM, 0.3 0.44650423728813554\n",
      "####INFO: trainning LinearSVM, 0.35 0.44841101694915253\n",
      "####INFO: trainning LinearSVM, 0.4 0.45042372881355935\n",
      "####INFO: trainning LinearSVM, 0.45 0.4513771186440678\n",
      "####INFO: trainning LinearSVM, 0.5 0.45116525423728815\n",
      "####INFO: trainning LinearSVM, 0.55 0.45243644067796607\n",
      "####INFO: trainning LinearSVM, 0.6 0.4526483050847457\n",
      "####INFO: trainning LinearSVM, 0.65 0.45201271186440684\n",
      "####INFO: trainning LinearSVM, 0.7 0.45201271186440684\n",
      "####INFO: trainning LinearSVM, 0.75 0.45233050847457623\n",
      "####INFO: trainning LinearSVM, 0.8 0.45233050847457623\n",
      "####INFO: trainning LinearSVM, 0.85 0.4516949152542374\n",
      "####INFO: trainning LinearSVM, 0.9 0.4514830508474576\n",
      "####INFO: trainning LinearSVM, 0.95 0.4515889830508476\n",
      "####INFO: trainning LinearSVM, 1.0 0.45116525423728815\n",
      "####INFO: trainning LinearSVM, 1.05 0.45127118644067793\n",
      "####INFO: trainning LinearSVM, 1.1 0.4507415254237288\n",
      "####INFO: trainning LinearSVM, 1.15 0.45052966101694925\n",
      "####INFO: trainning LinearSVM, 1.2 0.4501059322033899\n",
      "####INFO: trainning LinearSVM, 1.25 0.4496822033898306\n",
      "####INFO: trainning LinearSVM, 1.3 0.44936440677966116\n",
      "####INFO: trainning LinearSVM, 1.35 0.44894067796610165\n",
      "####INFO: trainning LinearSVM, 1.4 0.4483050847457627\n",
      "####INFO: trainning LinearSVM, 1.45 0.44819915254237275\n",
      "####INFO: trainning LinearSVM, 1.5 0.44788135593220335\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "raw_train_df = pd.read_csv(SML_TRAIN_FILE, delimiter='\\t', header=None, names=['ID','Text'])\n",
    "# raw_test_df = pd.read_csv(TEST_FILE, delimiter='\\t', header=None, names=['Text'])\n",
    "# print(train_df.shape)\n",
    "# print(test_df.shape)\n",
    "\n",
    "print(\"Finish reading\")\n",
    "\n",
    "preprocess_train_df = preprocess(raw_train_df, rmv_rt=True, rmv_all_spec=False, rmv_stop=False, lemmatize=False, add_pos=True)\n",
    "# preprocess_test_df = preprocess(raw_test_df, rmv_rt=False, rmv_all_spec=False, rmv_stop=False, lemmatize=False, add_pos=True)\n",
    "#print(preprocess_train_df.shape, preprocess_test_df.shape)\n",
    "\n",
    "print(\"Finsih preprocess\")\n",
    "\n",
    "'''\n",
    "merged_train_df = merge_tweets(preprocess_train_df)\n",
    "merged_test_df = preprocess_test_df\n",
    "merged_train_df['Text'] = merged_train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "merged_test_df['Text'] = merged_test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "\n",
    "tf_idf_train, tf_idf_test = tf_idf(merged_train_df, merged_test_df)\n",
    "print(tf_idf_train.shape, tf_idf_test.shape)\n",
    "print(tf_idf_train)\n",
    "'''\n",
    "\n",
    "cross_validate_tf_idf(preprocess_train_df, merge=False)\n",
    "\n",
    "# predict_tf_idf(preprocess_train_df, preprocess_test_df, merge=False)\n",
    "# print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
