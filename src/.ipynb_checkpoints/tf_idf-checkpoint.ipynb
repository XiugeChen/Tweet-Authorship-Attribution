{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "TRAIN_FILE = \"../resources/data/train_tweets.txt\"\n",
    "SML_TRAIN_FILE = \"../resources/data/test.txt\"\n",
    "TEST_FILE = \"../resources/data/test_tweets_unlabeled.txt\"\n",
    "\n",
    "GLOVE_25D = \"../resources/glove/glove.twitter.27B.25d.txt\"\n",
    "GLOVE_200D = \"../resources/glove/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all RTs (reweets)\n",
    "def filter_RT(df):\n",
    "    rt = df['Text'].str.startswith('RT @handle')\n",
    "    not_rt = [not i for i in rt]\n",
    "    result_df = df[not_rt]\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special terms like \"@handle\", links\n",
    "def rmv_special_term(df, rmv_all_spec=False):\n",
    "    # remove @s\n",
    "    result_df = df.replace(to_replace ='@handle', value = '@handle', regex=True)\n",
    "    # remove # but save tags\n",
    "    result_df = result_df.replace(to_replace ='#', value = '#', regex=True)\n",
    "    # remove links and urls\n",
    "    result_df = result_df.replace(to_replace ='\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', value = '@url', regex=True)\n",
    "    \n",
    "    # filter out all chars except 1-9/a-z/A-Z, such as :-( ' , . / \\ \n",
    "    if rmv_all_spec:\n",
    "        result_df = result_df.replace(to_replace ='([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', value = '', regex=True)\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# main call\n",
    "def preprocess(df, rmv_rt=True, rmv_all_spec=False, rmv_stop=False, lemmatize=False, word_ngram=[1], add_pos=False, pos_ngram=[1]):\n",
    "    logging.info('Preprocess starting')\n",
    "    \n",
    "    if rmv_rt:\n",
    "        df = filter_RT(df)\n",
    "    else:\n",
    "        df = df.replace(to_replace ='RT @handle', value = '@RT @handle', regex=True)\n",
    "    \n",
    "    result_df = rmv_special_term(df, rmv_all_spec)\n",
    "    \n",
    "    result_df['Text'] = result_df['Text'].apply(lambda x: x.rstrip().lstrip())\n",
    "    \n",
    "    # tokenize sentence\n",
    "    tknzr = TweetTokenizer()\n",
    "    result_df['Text'] = result_df['Text'].apply(lambda x: tknzr.tokenize(x))\n",
    "    \n",
    "    # add pos tags\n",
    "    if add_pos:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: pos_analysis(x, ngram=pos_ngram))\n",
    "        \n",
    "    if len(word_ngram) > 0:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: get_word_ngram(x, ngram=word_ngram))\n",
    "         \n",
    "    # remove stop words\n",
    "    if rmv_stop:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: [i for i in x if i not in cached_stop_words])\n",
    "    \n",
    "    # stem words\n",
    "    if lemmatize:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: [lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in x])\n",
    "    \n",
    "    logging.info('Preprocess ending')  \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis POS (part of speech) and ngram\n",
    "def pos_analysis(word_list, ngram=[1]):\n",
    "    if 0 in ngram:\n",
    "        return word_list\n",
    "    \n",
    "    postag_word = np.asarray(nltk.pos_tag(word_list))\n",
    "    \n",
    "    # get only pos tags\n",
    "    pos_tags = []\n",
    "    for chunk in postag_word:\n",
    "        pos_tags.append(chunk[1])\n",
    "    \n",
    "    result = word_list\n",
    "    for n in ngram:\n",
    "        pos_ngrams = []\n",
    "        \n",
    "        if n == 1:\n",
    "            pos_ngrams = pos_tags\n",
    "        else:\n",
    "            if n >= len(pos_tags):\n",
    "                pos_ngrams.append(''.join(tag for tag in pos_tags))\n",
    "            else:\n",
    "                for i in range(0, len(pos_tags)-n+1):\n",
    "                    new_tag = ''\n",
    "                    for j in range(i, i+n):\n",
    "                        new_tag += str(pos_tags[j])\n",
    "                \n",
    "                    pos_ngrams.append(new_tag)\n",
    "        \n",
    "        result = np.append(result, [i for i in pos_ngrams])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word ngram\n",
    "def get_word_ngram(raw_word_list, ngram=[1]):\n",
    "    result = []\n",
    "    # get pure word list, eliminate pos tags\n",
    "    pure_word_list = []\n",
    "    for word in raw_word_list:\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "        \n",
    "        if not word[0].isupper():\n",
    "            pure_word_list.append(word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "            \n",
    "    if 0 in ngram:\n",
    "        return result\n",
    "    \n",
    "    for n in ngram:\n",
    "        if n == 1:\n",
    "            result = np.append(result, pure_word_list)\n",
    "        else:\n",
    "            if n >= len(pure_word_list):\n",
    "                result = np.append(result, ''.join(word for word in pure_word_list))\n",
    "            else:\n",
    "                for i in range(0, len(pure_word_list)-n+1):\n",
    "                    new_tag = ''\n",
    "                    for j in range(i, i+n):\n",
    "                        new_tag += str(pure_word_list[j])\n",
    "                \n",
    "                    result = np.append(result, new_tag)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all tweets from same user\n",
    "def merge_tweets(df):\n",
    "    df['Text'] = df['Text'].apply(lambda x: ''.join(i + ' ' for i in x))\n",
    "    \n",
    "    aggregation_functions = {'Text': ''.join}\n",
    "    result_df = df.groupby(df['ID']).aggregate(aggregation_functions).reset_index()\n",
    "    \n",
    "    df['Text'] = df['Text'].apply(lambda x: x.rstrip())\n",
    "    print(\"finish merge tweets\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tf-idf features\n",
    "def tf_idf(train_df, test_df, min_df=1):\n",
    "    #get the text column \n",
    "    train_docs = train_df['Text'].tolist()\n",
    "    test_docs = test_df['Text'].tolist()\n",
    "    \n",
    "    #create a vocabulary of words, \n",
    "    #ignore words that appear in max_df of documents\n",
    "    cv = CountVectorizer(max_df=0.42, min_df=min_df, decode_error='ignore')\n",
    "    trian_wc_vec = cv.fit_transform(train_docs)\n",
    "    test_wc_vec = cv.transform(test_docs)\n",
    "    \n",
    "    # get tfidf\n",
    "    transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "    tfidf_train_df = transformer.fit_transform(trian_wc_vec)\n",
    "    tfidf_test_df = transformer.transform(test_wc_vec)\n",
    "    \n",
    "    #print(\"Finish tf-idf feature extraction\")\n",
    "\n",
    "    return tfidf_train_df, tfidf_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lexicon features\n",
    "def add_lexicon_features(df, feature_vec=None):\n",
    "    features = []\n",
    "    for index, row in df.iterrows():\n",
    "        text, feature = row['Text'], []\n",
    "        feature.extend(avg_var_word_len(text))\n",
    "        feature.append(len(text))\n",
    "        \n",
    "        features.append(feature)\n",
    "    \n",
    "    A = np.array(features)\n",
    "    A = normalize(A, axis=0, norm='max')\n",
    "    \n",
    "    if feature_vec == None:\n",
    "        print(\"finish add lexicon features\")\n",
    "        return sparse.csr_matrix(A)\n",
    "    else:\n",
    "        for column in A.T: \n",
    "            feature_vec = sparse.hstack((feature_vec, column[:,None]))\n",
    "        \n",
    "        print(\"finish add lexicon features\")\n",
    "        return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caculate averge word length for given sentence (word should starts with alphabet letters)\n",
    "def avg_var_word_len(text):\n",
    "    words, length = text.split(' '), []\n",
    "    \n",
    "    for word in words:\n",
    "        if word[0].isalpha() and word[0].islower():\n",
    "            length.append(len(word))\n",
    "            \n",
    "    length = np.array(length)\n",
    "    \n",
    "    if length.size == 0:\n",
    "        return [0, 0, 0]\n",
    "    \n",
    "    return [np.mean(length), np.std(length), np.median(length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate substring for df\n",
    "def generate_substring(df, length=6):\n",
    "    iter_df = df.copy()\n",
    "    \n",
    "    for index, row in iter_df.iterrows():\n",
    "        text, new_words = row['Text'], []\n",
    "        words = text.split(' ')\n",
    "        \n",
    "        for word in words: \n",
    "            if len(word) >= length:\n",
    "                for i in range(0, len(word)-length+1):\n",
    "                    sub_word = word[i:i + length]\n",
    "                    new_words.append(sub_word)\n",
    "                \n",
    "            new_words.append(word)\n",
    "        \n",
    "        new_text = ''.join(i + ' ' for i in new_words).rstrip()\n",
    "        \n",
    "        df.loc[index, 'Text'] = new_text\n",
    "    \n",
    "    #print(\"Finish substring extraction\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# sentiment analysis\n",
    "def sentiment_analysis(df, feature_vec=None):\n",
    "    sentiment_scores = []\n",
    "    for index, row in df.iterrows():\n",
    "        text, ori_words = row['Text'], []\n",
    "        words = text.split(' ')\n",
    "        \n",
    "        for word in words:\n",
    "            if '@rt' in word:\n",
    "                break\n",
    "                \n",
    "            if len(word) < 1:\n",
    "                continue\n",
    "                \n",
    "            ori_words.append(word)\n",
    "                \n",
    "        sentence = ''.join(i + ' ' for i in ori_words).rstrip()\n",
    "        score = analyser.polarity_scores(sentence)\n",
    "        sentiment_scores.append([score.get('neg'), score.get('neu'), score.get('pos'), score.get('compound')])\n",
    "        \n",
    "    A = np.array(sentiment_scores)\n",
    "    if feature_vec == None:\n",
    "        #print(\"finish add sentiment features\")\n",
    "        return sparse.csr_matrix(A)\n",
    "    else:\n",
    "        for column in A.T: \n",
    "            feature_vec = sparse.hstack((feature_vec, column[:,None]))\n",
    "        \n",
    "        #print(\"finish add sentiment features\")\n",
    "        return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduction(X_train, X_test):\n",
    "    pca = TruncatedSVD(n_components=1000)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    print(\"finish pca reduction\")\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "models = [svm.LinearSVC(C=0.68, max_iter=1000)]\n",
    "         #SGDClassifier(loss=\"squared_hinge\", penalty=\"l2\", max_iter=1000, n_jobs=-1, tol=1e-4)]\n",
    "          #LogisticRegression(solver=\"lbfgs\", penalty=\"l2\", C=1e-2, multi_class='auto', max_iter=1000, fit_intercept= False)]\n",
    "         #XGBClassifier(random_state=42, seed=2, colsample_bytree=0.6, subsample=0.7, max_depth=3, n_estimators=300, learning_rate=0.1)]\n",
    "         # MultinomialNB()]\n",
    "\n",
    "titles = ['LinearSVM, 1']\n",
    "          #'SGDClassifier']\n",
    "          #'LogisticRegression']\n",
    "         #'XGBoost']\n",
    "          #'MNB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_tf_idf(df, merge=False, add_lexicon=False, substring=False, substring_len=3, add_sentiment=False, pca=False):\n",
    "    cv = KFold(n_splits=10, random_state=90051, shuffle=True)\n",
    "    \n",
    "    scores = {}\n",
    "    for train_index, test_index in cv.split(df):\n",
    "        print(\"Batch start\")\n",
    "        train_df, test_df = df.iloc[train_index].reset_index(drop=True), df.iloc[test_index].reset_index(drop=True)\n",
    "        \n",
    "        # merge all tweets from same user to one document string\n",
    "        if merge:\n",
    "            train_df = merge_tweets(train_df)\n",
    "        else:\n",
    "            train_df['Text'] = train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "        test_df['Text'] = test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "        if substring:\n",
    "            train_df = generate_substring(train_df, length=substring_len)\n",
    "            test_df = generate_substring(test_df, length=substring_len)\n",
    "        \n",
    "        X_train, X_test = tf_idf(train_df, test_df)\n",
    "        y_train, y_test = train_df['ID'], test_df['ID']\n",
    "        \n",
    "        if add_lexicon:\n",
    "            X_train = add_lexicon_features(train_df, feature_vec=X_train)\n",
    "            X_test = add_lexicon_features(test_df, feature_vec=X_test)\n",
    "            \n",
    "        if add_sentiment:\n",
    "            X_train = sentiment_analysis(train_df, feature_vec=X_train)\n",
    "            X_test = sentiment_analysis(test_df, feature_vec=X_test)\n",
    "            \n",
    "        if pca:\n",
    "            X_train, X_test = pca_reduction(X_train, X_test)\n",
    "        \n",
    "        for title, model in zip(titles, models):\n",
    "            print(\"Start train \" + title)\n",
    "            model.fit(X_train, y_train)\n",
    "            predicted_labels = model.predict(X_test)\n",
    "            acc = accuracy_score(predicted_labels, y_test)\n",
    "            print(title + \" \" + str(acc))\n",
    "            \n",
    "            # uncomment to print miss labeled data\n",
    "            # for i in range(0, len(predicted_labels)):\n",
    "            #   if predicted_labels[i] != y_test[i]:\n",
    "            #        print(\"#\" + str(i) + \"; T: \" + str(y_test[i]) + \"; F: \" + str(predicted_labels[i]) + \"; Text: \" + test_df.loc[i,'Text'])\n",
    "            \n",
    "            if title in scores.keys():\n",
    "                scores[title] += acc\n",
    "            else:\n",
    "                scores[title] = acc\n",
    "        \n",
    "    for title in titles:\n",
    "        acc = scores[title] / 10\n",
    "        print(\"####INFO: trainning\", title, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf(train_df, test_df, merge=False, add_lexicon=False, substring=False, substring_len=3, add_sentiment=False):\n",
    "    # merge all tweets from same user to one document string\n",
    "    if merge:\n",
    "        train_df = merge_tweets(train_df)\n",
    "    else:\n",
    "        train_df['Text'] = train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "    test_df['Text'] = test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "    \n",
    "    if substring:\n",
    "        train_df = generate_substring(train_df, length=substring_len)\n",
    "        test_df = generate_substring(test_df, length=substring_len)\n",
    "        \n",
    "    X_train, X_test = tf_idf(train_df, test_df)\n",
    "    y_train = train_df['ID']\n",
    "    \n",
    "    if add_lexicon:\n",
    "        X_train = add_lexicon_features(train_df, feature_vec=X_train)\n",
    "        X_test = add_lexicon_features(test_df, feature_vec=X_test)\n",
    "        \n",
    "    if add_sentiment:\n",
    "        X_train = sentiment_analysis(train_df, feature_vec=X_train)\n",
    "        X_test = sentiment_analysis(test_df, feature_vec=X_test)\n",
    "    \n",
    "    for title, model in zip(titles, models):\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"model \" + title + \" fit finished\")\n",
    "        label = model.predict(X_test)\n",
    "        print(\"model predict finished\")\n",
    "    \n",
    "        wtr = csv.writer(open('prediction_' + title + '.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "        wtr.writerow(['Id','Predicted'])\n",
    "        for i in range(0, label.size):\n",
    "            wtr.writerow([i+1, label[i]])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, train_df, test_df, wordngram=[1], pos=False, posngram=[1], addsentiment=True, min_tf_idf=1):\n",
    "    train_df = preprocess(train_df, rmv_rt=False, rmv_all_spec=False, rmv_stop=False, lemmatize=False, word_ngram=wordngram, add_pos=pos, pos_ngram=posngram)\n",
    "    test_df = preprocess(test_df, rmv_rt=False, rmv_all_spec=False, rmv_stop=False, lemmatize=False, word_ngram=wordngram, add_pos=pos, pos_ngram=posngram)\n",
    "    \n",
    "    train_df['Text'] = train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "    test_df['Text'] = test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "    \n",
    "    X_train, X_test = tf_idf(train_df, test_df, min_df=min_tf_idf)\n",
    "    y_train, y_test = train_df['ID'], test_df['ID']\n",
    "    \n",
    "    if addsentiment:\n",
    "        X_train = sentiment_analysis(train_df, feature_vec=X_train)\n",
    "        X_test = sentiment_analysis(test_df, feature_vec=X_test)\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "    train_labels = model.predict(X_train)\n",
    "    predicted_labels = model.predict(X_test)\n",
    "    \n",
    "    return np.array(train_labels).reshape(-1,1), np.array(predicted_labels).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_cross_validate(raw_df, add_sentiment=True):\n",
    "    cv = KFold(n_splits=10, random_state=90051, shuffle=True)\n",
    "    \n",
    "    score = 0\n",
    "    for train_index, test_index in cv.split(raw_df):\n",
    "        train_df, test_df = raw_df.iloc[train_index].reset_index(drop=True), raw_df.iloc[test_index].reset_index(drop=True)\n",
    "        y_train, y_test = train_df['ID'], test_df['ID']\n",
    "        \n",
    "        svm_model = svm.LinearSVC(C=0.68, max_iter=1000)\n",
    "        \n",
    "        train_1, test_1 = predict(svm_model, train_df, test_df, wordngram=[1], pos=True, posngram=[1], addsentiment=True, min_tf_idf=1)\n",
    "        train_2, test_2 = predict(svm_model, train_df, test_df, wordngram=[2], pos=False, posngram=[1], addsentiment=True, min_tf_idf=1)\n",
    "        train_3, test_3 = predict(svm_model, train_df, test_df, wordngram=[1], pos=True, posngram=[1000], addsentiment=True, min_tf_idf=1)\n",
    "        \n",
    "        h_model = svm.LinearSVC(C=0.68, max_iter=1000)\n",
    "        \n",
    "        X_train, X_test = [], []\n",
    "        for i in range(0, len(train_1)):\n",
    "            X_train.append(str(train_1[i]) + ' ' + str(train_2[i]) + ' ' + str(train_3[i]))\n",
    "        for i in range(0, len(test_1)):\n",
    "            X_test.append(str(test_1[i]) + ' ' + str(test_2[i]) + ' ' + str(test_3[i]))\n",
    "        \n",
    "        X_train = np.array(X_train)\n",
    "        X_test = np.array(X_test)\n",
    "        \n",
    "        stop_words = stopwords.words('english')\n",
    "        cv = CountVectorizer(max_df=0.85, stop_words=stop_words, decode_error='ignore')\n",
    "        trian_wc_vec = cv.fit_transform(X_train)\n",
    "        test_wc_vec = cv.transform(X_test)\n",
    "    \n",
    "        # get tfidf\n",
    "        transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "        X_train = transformer.fit_transform(trian_wc_vec)\n",
    "        X_test = transformer.transform(test_wc_vec)\n",
    "        \n",
    "        h_model.fit(X_train, y_train)\n",
    "        train_acc = accuracy_score(h_model.predict(X_train), y_train)\n",
    "        \n",
    "        predicted_labels = h_model.predict(X_test)\n",
    "        acc = accuracy_score(predicted_labels, y_test)\n",
    "        \n",
    "        sub_acc_1, sub_acc_2, sub_acc_3 = accuracy_score(train_1, y_train), accuracy_score(train_2, y_train), accuracy_score(train_3, y_train)\n",
    "        #print(\"####INFO train error: \", train_acc, sub_acc_1, sub_acc_2, sub_acc_3)\n",
    "        sub_acc_1, sub_acc_2, sub_acc_3 = accuracy_score(test_1, y_test), accuracy_score(test_2, y_test), accuracy_score(test_3, y_test)\n",
    "        #print(\"####INFO test error: \", acc, sub_acc_1, sub_acc_2, sub_acc_3)\n",
    "        \n",
    "        # uncomment to print miss labeled data\n",
    "        # for i in range(0, len(predicted_labels)):\n",
    "        #   if predicted_labels[i] != y_test[i]:\n",
    "        #        print(\"#\" + str(i) + \"; T: \" + str(y_test[i]) + \"; F: \" + str(predicted_labels[i]) + \"; Text: \" + test_df.loc[i,'Text'])\n",
    "        \n",
    "        score += acc\n",
    "        \n",
    "    avg_acc = score / 10\n",
    "    print(\"####INFO: trainning\", 'Stacking', avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading\n",
      "Finsih preprocess\n",
      "Batch start\n",
      "Start train LinearSVM, 1\n",
      "LinearSVM, 1 0.34166515179058354\n",
      "Start train SGDClassifier\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4d5f6e1ff35f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finsih preprocess\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcross_validate_tf_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_lexicon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstring_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_sentiment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#stacking_cross_validate(raw_train_df, add_sentiment=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-97a68f89ec1c>\u001b[0m in \u001b[0;36mcross_validate_tf_idf\u001b[0;34m(df, merge, add_lexicon, substring, substring_len, add_sentiment, pca)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start train \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    746\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m                          sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         self._partial_fit(X, y, alpha, C, loss, learning_rate, self._max_iter,\n\u001b[0;32m--> 596\u001b[0;31m                           classes, sample_weight, coef_init, intercept_init)\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         if (self._tol is not None and self._tol > -np.inf\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m    548\u001b[0m                                  \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m                                  max_iter=max_iter)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             self._fit_binary(X, y, alpha=alpha, C=C,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit_multiclass\u001b[0;34m(self, X, y, alpha, C, learning_rate, sample_weight, max_iter)\u001b[0m\n\u001b[1;32m    648\u001b[0m                                 \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                                 validation_mask=validation_mask)\n\u001b[0;32m--> 650\u001b[0;31m             for i in range(len(self.classes_)))\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;31m# take the maximum of n_iter_ over every binary fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "raw_train_df = pd.read_csv(SML_TRAIN_FILE, delimiter='\\t', header=None, names=['ID','Text'])\n",
    "# raw_test_df = pd.read_csv(TEST_FILE, delimiter='\\t', header=None, names=['Text'])\n",
    "# print(train_df.shape)\n",
    "# print(test_df.shape)\n",
    "\n",
    "print(\"Finish reading\")\n",
    "preprocess_train_df = preprocess(raw_train_df, rmv_rt=False, rmv_all_spec=False, rmv_stop=False, lemmatize=False, word_ngram=[1], add_pos=True, pos_ngram=[1])\n",
    "# preprocess_test_df = preprocess(raw_test_df, rmv_rt=False, rmv_all_spec=False, rmv_stop=False, lemmatize=False, word_ngram=2, add_pos=True, pos_ngram=1000)\n",
    "#print(preprocess_train_df.shape, preprocess_test_df.shape)\n",
    "\n",
    "print(\"Finsih preprocess\")\n",
    "cross_validate_tf_idf(preprocess_train_df, merge=False, add_lexicon=False, substring=False, substring_len=3, add_sentiment=True, pca=False)\n",
    "\n",
    "#stacking_cross_validate(raw_train_df, add_sentiment=True)\n",
    "\n",
    "#predict_tf_idf(preprocess_train_df, preprocess_test_df, merge=False, add_lexicon=False, substring=False, substring_len=3, add_sentiment=True)\n",
    "#print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####INFO: trainning LinearSVM, 0.68 0.45655970336003937"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
