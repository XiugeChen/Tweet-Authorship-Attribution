{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import logging\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "TRAIN_FILE = \"../resources/data/train_tweets.txt\"\n",
    "TEST_FILE = \"../resources/data/test_tweets_unlabeled.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILE, delimiter='\\t', header=None, names=['ID','Text'])\n",
    "test_df = pd.read_csv(TEST_FILE, delimiter='\\t', header=None, names=['Text'])\n",
    "\n",
    "# print(train_df.shape)\n",
    "# print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all RTs (reweets)\n",
    "def filter_RT(df):\n",
    "    rt = df['Text'].str.startswith('RT @handle')\n",
    "    not_rt = [not i for i in rt]\n",
    "    result_df = df[not_rt]\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special terms like \"@handle\", links\n",
    "def rmv_special_term(df, rmv_all_spec=False):\n",
    "    # remove @s\n",
    "    result_df = df.replace(to_replace ='@handle', value = '', regex=True)\n",
    "    # remove links and urls\n",
    "    result_df = result_df.replace(to_replace ='\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', value = '', regex=True)\n",
    "    \n",
    "    # filter out all chars except 1-9/a-z/A-Z, such as :-( ' , . / \\ \n",
    "    if rmv_all_spec:\n",
    "        result_df = result_df.replace(to_replace ='([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', value = '', regex=True)\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# main call\n",
    "def preprocess(df, rmv_all_spec=False, rmv_stop=False, lemmatize=False):\n",
    "    logging.info('Preprocess starting')\n",
    "    \n",
    "    result_df = rmv_special_term(filter_RT(df), rmv_all_spec)\n",
    "    \n",
    "    #result_df['Text'] = result_df['Text'].str.lower()\n",
    "    result_df['Text'] = result_df['Text'].apply(lambda x: x.lower().rstrip().lstrip())\n",
    "    \n",
    "    # tokenize sentence\n",
    "    tknzr = TweetTokenizer()\n",
    "    result_df['Text'] = result_df['Text'].apply(lambda x: tknzr.tokenize(x))\n",
    "        \n",
    "    # remove stop words\n",
    "    if rmv_stop:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: [i for i in x if i not in cached_stop_words])\n",
    "    \n",
    "    # stem words\n",
    "    if lemmatize:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: [lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in x])\n",
    "    \n",
    "    logging.info('Preprocess ending')  \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307679, 2)\n",
      "          ID                                               Text\n",
      "0       8746                [let, try, catch, live, next, week]\n",
      "1       8746  [go, watch, grey, big, screen, thursday, indul...\n",
      "2       8746                      [pleasure, patrickhope, well]\n",
      "3       8746  [hi, travel, lot, lot, come, next, month, reco...\n",
      "4       8746                           [u, get, invite, justin]\n",
      "5       8746                  [think, still, good, friend, lol]\n",
      "6       8746                    [remember, fine, u, whats, new]\n",
      "7       8746                        [thats, great, good, coach]\n",
      "8       8746     [dont, want, picture, u, sit, lol, understand]\n",
      "9       8746                [thanks, rtsare, go, womma, summit]\n",
      "10      8746                         [grrryou, must, go, crazy]\n",
      "11      8746                      [hi, catch, trip, news, dale]\n",
      "12      8746            [still, carwant, jump, 45, minute, eta]\n",
      "13      8746  [wish, could, 247, w, stus, family, drive, hom...\n",
      "14      8746                                        [yum, save]\n",
      "15      8746                                [dont, think, know]\n",
      "16      8746                         [enter, ohio, special, hi]\n",
      "17      8746  [well, agree, one, food, thing, friend, pumpki...\n",
      "18      8746                                                [1]\n",
      "19      8746                              [4, hour, til, akron]\n",
      "20      8746  [drive, ohiolook, fforward, respond, sunhave, ...\n",
      "21      8746                                        [ate, king]\n",
      "22      8746    [u, tough, chick, hope, ruin, banquet, u, dale]\n",
      "23      8746  [let, jake, take, one, last, week, half, dayth...\n",
      "24      8746  [tennesee, say, hi, back, watch, bk, sure, lun...\n",
      "25      8746  [ah, hahschools, close, atlanta, todayonly, 8,...\n",
      "26      8746        [hi, lil, drive, thru, tennessee, p, early]\n",
      "27      8746                [glad, u, remember, great, holiday]\n",
      "28      8746             [thank, rob, proud, site, happy, tday]\n",
      "29      8746  [night, happy, thaksgiving, friend, old, new, ...\n",
      "...      ...                                                ...\n",
      "307649  1319                                                 []\n",
      "307650  1319       [please, add, awsms, 09, afterparty, thanks]\n",
      "307651  1319   [great, party, last, nightmet, talented, people]\n",
      "307652  1319  [alta, phoenix, loft, 1, phoenix, congrats, in...\n",
      "307653  9235                      [manage, thing, lead, people]\n",
      "307654  9235                       [know, bad, wish, know, bad]\n",
      "307655  9235  [one, man, die, ignorant, capacity, knowledge,...\n",
      "307656  9235                                [character, action]\n",
      "307657  9235              [mind, dwells, upon, body, act, upon]\n",
      "307658  9235                       [success, see, result, goal]\n",
      "307659  9235              [generalization, false, include, one]\n",
      "307660  9235  [province, knowledge, speak, privilege, wisdom...\n",
      "307661  9235  [leader, convince, particular, course, action,...\n",
      "307662  9235                        [make, excuse, money, time]\n",
      "307663  4357  [henry, brother, electronics, inc, participate...\n",
      "307664  4357  [techinsights, esc, uk, event, showcase, lead,...\n",
      "307665  4357  [demofall, 09, announces, lineup, emerge, tech...\n",
      "307666  4357  [afp, host, symposium, essential, business, ch...\n",
      "307667  4357  [alertenterprise, win, asis, accolade, 2009, s...\n",
      "307668  4357  [andrew, international, introduces, new, metho...\n",
      "307669  4357  [innovation, strong, despite, recession, human...\n",
      "307670  4357  [videoiq, milestone, system, partner, deliver,...\n",
      "307671  4357  [phoenix, technology, showcase, cut, edge, tec...\n",
      "307672  4357  [anydatas, apt, 210, track, device, measure, a...\n",
      "307673  4357  [samplify, system, announces, distribution, ag...\n",
      "307674  4357  [steelbox, demonstrates, open, video, framewor...\n",
      "307675  4357  [small, business, rely, sage, help, ride, rece...\n",
      "307676  4357  [timesight, system, announces, nextgeneration,...\n",
      "307677  4357  [diebold, make, lead, monitoring, solution, av...\n",
      "307678  4357  [gvi, security, solution, introduce, autoip, v...\n",
      "\n",
      "[307679 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "new_train_df = preprocess(train_df)\n",
    "print(new_train_df.shape)\n",
    "print(new_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
