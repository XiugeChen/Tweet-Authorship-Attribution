{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import logging\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "TRAIN_FILE = \"../resources/data/train_tweets.txt\"\n",
    "SML_TRAIN_FILE = \"../resources/data/test.txt\"\n",
    "TEST_FILE = \"../resources/data/test_tweets_unlabeled.txt\"\n",
    "\n",
    "GLOVE_25D = \"../resources/glove/glove.twitter.27B.25d.txt\"\n",
    "GLOVE_200D = \"../resources/glove/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all RTs (reweets)\n",
    "def filter_RT(df):\n",
    "    rt = df['Text'].str.startswith('RT @handle')\n",
    "    not_rt = [not i for i in rt]\n",
    "    result_df = df[not_rt]\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special terms like \"@handle\", links\n",
    "def rmv_special_term(df, rmv_all_spec=False):\n",
    "    # remove @s\n",
    "    result_df = df.replace(to_replace ='@handle', value = '', regex=True)\n",
    "    # remove # but save tags\n",
    "    result_df = result_df.replace(to_replace ='#', value = '', regex=True)\n",
    "    # remove links and urls\n",
    "    result_df = result_df.replace(to_replace ='\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', value = '', regex=True)\n",
    "    \n",
    "    # filter out all chars except 1-9/a-z/A-Z, such as :-( ' , . / \\ \n",
    "    if rmv_all_spec:\n",
    "        result_df = result_df.replace(to_replace ='([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', value = '', regex=True)\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# main call\n",
    "def preprocess(df, rmv_all_spec=False, rmv_stop=False, lemmatize=False):\n",
    "    logging.info('Preprocess starting')\n",
    "    \n",
    "    result_df = rmv_special_term(filter_RT(df), rmv_all_spec)\n",
    "    \n",
    "    #result_df['Text'] = result_df['Text'].str.lower()\n",
    "    result_df['Text'] = result_df['Text'].apply(lambda x: x.lower().rstrip().lstrip())\n",
    "    \n",
    "    # tokenize sentence\n",
    "    tknzr = TweetTokenizer()\n",
    "    result_df['Text'] = result_df['Text'].apply(lambda x: tknzr.tokenize(x))\n",
    "        \n",
    "    # remove stop words\n",
    "    if rmv_stop:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: [i for i in x if i not in cached_stop_words])\n",
    "    \n",
    "    # stem words\n",
    "    if lemmatize:\n",
    "        result_df['Text'] = result_df['Text'].apply(lambda x: [lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in x])\n",
    "    \n",
    "    logging.info('Preprocess ending')  \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all tweets from same user\n",
    "def merge_tweets(df):\n",
    "    aggregation_functions = {'Text': 'sum'}\n",
    "    result_df = df.groupby(df['ID']).aggregate(aggregation_functions).reset_index()\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tf-idf features\n",
    "def tf_idf(train_df, test_df):\n",
    "    #get the text column \n",
    "    train_docs = train_df['Text'].tolist()\n",
    "    test_docs = test_df['Text'].tolist()\n",
    "    \n",
    "    #create a vocabulary of words, \n",
    "    #ignore words that appear in max_df of documents\n",
    "    stop_words = stopwords.words('english')\n",
    "    cv = CountVectorizer(max_df=0.85, stop_words=stop_words, decode_error='ignore')\n",
    "    trian_wc_vec = cv.fit_transform(train_docs)\n",
    "    test_wc_vec = cv.transform(test_docs)\n",
    "    \n",
    "    # get tfidf\n",
    "    transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "    tfidf_train_df = transformer.fit_transform(trian_wc_vec)\n",
    "    tfidf_test_df = transformer.transform(test_wc_vec)\n",
    "\n",
    "    return tfidf_train_df, tfidf_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract word vector features\n",
    "def wordvec(train_df, test_df, sizeD=25):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "models = [svm.LinearSVC(C=1, max_iter=1000),\n",
    "          MultinomialNB(),\n",
    "          ]\n",
    "\n",
    "titles = ['LinearSVM',\n",
    "          'MNB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_tf_idf(df):\n",
    "    cv = KFold(n_splits=10, random_state=90051, shuffle=True)\n",
    "    \n",
    "    scores = {}\n",
    "    for train_index, test_index in cv.split(df):\n",
    "        train_df, test_df = df.iloc[train_index].reset_index(drop=True), df.iloc[test_index].reset_index(drop=True)\n",
    "        \n",
    "        # merge all tweets from same user to one document string\n",
    "        train_df = merge_tweets(train_df)\n",
    "        \n",
    "        train_df['Text'] = train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        test_df['Text'] = test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "        X_train, X_test = tf_idf(train_df, test_df)\n",
    "        y_train, y_test = train_df['ID'], test_df['ID']\n",
    "        \n",
    "        for title, model in zip(titles, models):\n",
    "            model.fit(X_train, y_train)\n",
    "            acc = accuracy_score(model.predict(X_test), y_test)\n",
    "            \n",
    "            if title in scores.keys():\n",
    "                scores[title] += acc\n",
    "            else:\n",
    "                scores[title] = acc\n",
    "        \n",
    "    for title in titles:\n",
    "        acc = scores[title] / 10\n",
    "        print(\"####INFO: trainning\", title, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf(train_df, test_df):\n",
    "    # merge all tweets from same user to one document string\n",
    "    train_df = merge_tweets(train_df)\n",
    "        \n",
    "    train_df['Text'] = train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "    test_df['Text'] = test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "        \n",
    "    X_train, X_test = tf_idf(train_df, test_df)\n",
    "    y_train = train_df['ID']\n",
    "    \n",
    "    for title, model in zip(titles, models):\n",
    "            model.fit(X_train, y_train)\n",
    "            label = model.predict(X_test)\n",
    "    \n",
    "    wtr = csv.writer(open('prediction.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "    wtr.writerow(['Id','Predicted'])\n",
    "    for i in range(0, label.length):\n",
    "        wtr.writerow([i, label[i]])\n",
    "        print(i + ',' + label[i])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####INFO: trainning LinearSVM 0.26045960428689197\n",
      "####INFO: trainning MNB 0.27161995053586147\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "raw_train_df = pd.read_csv(SML_TRAIN_FILE, delimiter='\\t', header=None, names=['ID','Text'])\n",
    "#raw_test_df = pd.read_csv(TEST_FILE, delimiter='\\t', header=None, names=['Text'])\n",
    "# print(train_df.shape)\n",
    "# print(test_df.shape)\n",
    "\n",
    "preprocess_train_df = preprocess(raw_train_df, rmv_all_spec=False, rmv_stop=True, lemmatize=True)\n",
    "#preprocess_test_df = preprocess(raw_test_df, rmv_all_spec=True, rmv_stop=True, lemmatize=True)\n",
    "#print(preprocess_train_df.shape, preprocess_test_df.shape)\n",
    "\n",
    "'''\n",
    "merged_train_df = merge_tweets(preprocess_train_df)\n",
    "merged_test_df = preprocess_test_df\n",
    "merged_train_df['Text'] = merged_train_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "merged_test_df['Text'] = merged_test_df['Text'].apply(lambda x: ''.join(i + ' ' for i in x).rstrip())\n",
    "\n",
    "tf_idf_train, tf_idf_test = tf_idf(merged_train_df, merged_test_df)\n",
    "print(tf_idf_train.shape, tf_idf_test.shape)\n",
    "print(tf_idf_train)\n",
    "'''\n",
    "\n",
    "cross_validate_tf_idf(preprocess_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
